{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 5\n",
    "\n",
    "Harsh Kashyap\n",
    "101917088\n",
    "CSE4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abdallah</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>...</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipline</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 6594 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                aa  ab  abandoned  abdallah  abducted  abhorring  abiding  \\\n",
       "Amer_Mohammed    0   1          0         1         0          0        0   \n",
       "Dave_2           0   0          0         0         0          0        2   \n",
       "Dave_Chappelle   0   0          0         0         0          0        0   \n",
       "Drew_Michael_1   1   0          0         0         0          0        0   \n",
       "Drew_Michael_2   0   0          1         0         0          0        0   \n",
       "Jim_Gaffigan     0   0          0         0         0          0        0   \n",
       "Kathleen         0   0          0         0         1          1        0   \n",
       "Lousi            0   0          0         0         0          0        0   \n",
       "Phil             0   0          1         0         0          0        0   \n",
       "Tom              0   0          0         0         0          0        1   \n",
       "\n",
       "                ability  able  aboard  ...  yuck  yup  zero  zimbabwe  \\\n",
       "Amer_Mohammed         0     1       0  ...     0    2     0         0   \n",
       "Dave_2                0     0       0  ...     1    0     0         0   \n",
       "Dave_Chappelle        0     3       0  ...     0    0     0         0   \n",
       "Drew_Michael_1        2     2       0  ...     0    0     0         0   \n",
       "Drew_Michael_2        1     5       0  ...     0    0     1         0   \n",
       "Jim_Gaffigan          0     3       0  ...     1    0     0         0   \n",
       "Kathleen              0     1       0  ...     0    0     0         0   \n",
       "Lousi                 0     1       0  ...     0    0     0         2   \n",
       "Phil                  0     0       0  ...     0    0     0         0   \n",
       "Tom                   0     0       1  ...     0    0     0         0   \n",
       "\n",
       "                zimmerman  zip  zipline  ziplining  zone  zoo  \n",
       "Amer_Mohammed           0    0        1          1     0    0  \n",
       "Dave_2                  6    0        0          0     0    0  \n",
       "Dave_Chappelle          0    0        0          0     0    0  \n",
       "Drew_Michael_1          0    0        0          0     0    0  \n",
       "Drew_Michael_2          0    0        0          0     0    0  \n",
       "Jim_Gaffigan            0    0        0          3     0    0  \n",
       "Kathleen                0    1        0          0     0    0  \n",
       "Lousi                   0    0        0          0     0    9  \n",
       "Phil                    0    0        0          0     0    0  \n",
       "Tom                     0    0        0          0     1    0  \n",
       "\n",
       "[10 rows x 6594 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_pickle('pickle\\dtm_stop.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <th>Dave_2</th>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <th>Kathleen</th>\n",
       "      <th>Lousi</th>\n",
       "      <th>Phil</th>\n",
       "      <th>Tom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdallah</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abducted</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Amer_Mohammed  Dave_2  Dave_Chappelle  Drew_Michael_1  \\\n",
       "aa                     0       0               0               1   \n",
       "ab                     1       0               0               0   \n",
       "abandoned              0       0               0               0   \n",
       "abdallah               1       0               0               0   \n",
       "abducted               0       0               0               0   \n",
       "\n",
       "           Drew_Michael_2  Jim_Gaffigan  Kathleen  Lousi  Phil  Tom  \n",
       "aa                      0             0         0      0     0    0  \n",
       "ab                      0             0         0      0     0    0  \n",
       "abandoned               1             0         0      0     1    0  \n",
       "abdallah                0             0         0      0     0    0  \n",
       "abducted                0             0         1      0     0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = df.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3246: 'ladies',\n",
       " 2416: 'gentleman',\n",
       " 2123: 'feel',\n",
       " 1984: 'excitement',\n",
       " 122: 'air',\n",
       " 2815: 'houston',\n",
       " 5845: 'texas',\n",
       " 3337: 'lets',\n",
       " 4902: 'road',\n",
       " 1797: 'dwayne',\n",
       " 3123: 'johnson',\n",
       " 3229: 'known',\n",
       " 4912: 'rock',\n",
       " 3238: 'la',\n",
       " 4911: 'roca',\n",
       " 5415: 'spanish',\n",
       " 5134: 'sexiest',\n",
       " 3761: 'motherfucker',\n",
       " 147: 'alive',\n",
       " 2899: 'important',\n",
       " 2573: 'guys',\n",
       " 1147: 'come',\n",
       " 3522: 'man',\n",
       " 2586: 'hails',\n",
       " 4334: 'place',\n",
       " 142: 'alief',\n",
       " 275: 'area',\n",
       " 6569: 'yes',\n",
       " 5722: 'swat',\n",
       " 169: 'amazing',\n",
       " 3404: 'lizzo',\n",
       " 535: 'beyoncé',\n",
       " 6038: 'travis',\n",
       " 5049: 'scott',\n",
       " 4251: 'person',\n",
       " 1591: 'did',\n",
       " 4231: 'peoples',\n",
       " 935: 'champ',\n",
       " 5962: 'tonight',\n",
       " 3065: 'ive',\n",
       " 5957: 'tomorrow',\n",
       " 2873: 'ill',\n",
       " 4517: 'probably',\n",
       " 2654: 'hawaii',\n",
       " 3391: 'listen',\n",
       " 2783: 'honor',\n",
       " 3026: 'introducing',\n",
       " 2417: 'gentlemen',\n",
       " 6361: 'warm',\n",
       " 2767: 'hometown',\n",
       " 6413: 'welcome',\n",
       " 937: 'champion',\n",
       " 673: 'boy',\n",
       " 3717: 'mo',\n",
       " 175: 'amer',\n",
       " 6429: 'whats',\n",
       " 5074: 'seat',\n",
       " 6550: 'yall',\n",
       " 4704: 'real',\n",
       " 3587: 'mean',\n",
       " 3743: 'months',\n",
       " 6423: 'weve',\n",
       " 3398: 'live',\n",
       " 4241: 'performer',\n",
       " 5887: 'thought',\n",
       " 2775: 'honest',\n",
       " 5078: 'second',\n",
       " 853: 'career',\n",
       " 4439: 'potentially',\n",
       " 3821: 'named',\n",
       " 3725: 'mohammed',\n",
       " 3430: 'looking',\n",
       " 2473: 'good',\n",
       " 3576: 'matter',\n",
       " 2050: 'fact',\n",
       " 2070: 'family',\n",
       " 6098: 'turned',\n",
       " 719: 'brother',\n",
       " 3428: 'looked',\n",
       " 1475: 'dead',\n",
       " 2043: 'eyes',\n",
       " 2461: 'goes',\n",
       " 5018: 'say',\n",
       " 6356: 'wanted',\n",
       " 5491: 'stand',\n",
       " 1149: 'comedian',\n",
       " 5763: 'talking',\n",
       " 3883: 'news',\n",
       " 6358: 'wants',\n",
       " 5484: 'stage',\n",
       " 5814: 'telling',\n",
       " 3133: 'jokes',\n",
       " 4982: 'said',\n",
       " 4015: 'omar',\n",
       " 4313: 'pilot',\n",
       " 3780: 'movie',\n",
       " 2046: 'face',\n",
       " 575: 'bitch',\n",
       " 6530: 'wow',\n",
       " 4134: 'pandemic',\n",
       " 310: 'asking',\n",
       " 3280: 'laughing',\n",
       " 3731: 'moment',\n",
       " 2964: 'inside',\n",
       " 1672: 'divorce',\n",
       " 2342: 'fuck',\n",
       " 1673: 'divorced',\n",
       " 2622: 'happened',\n",
       " 2628: 'happy',\n",
       " 5571: 'stories',\n",
       " 2666: 'hear',\n",
       " 1150: 'comedians',\n",
       " 1592: 'didnt',\n",
       " 6504: 'work',\n",
       " 2176: 'fine',\n",
       " 3553: 'married',\n",
       " 3423: 'long',\n",
       " 6236: 'use',\n",
       " 924: 'certain',\n",
       " 5873: 'things',\n",
       " 312: 'asleep',\n",
       " 3903: 'night',\n",
       " 2087: 'farted',\n",
       " 2631: 'hard',\n",
       " 6486: 'woke',\n",
       " 301: 'ashamed',\n",
       " 3318: 'left',\n",
       " 233: 'apologized',\n",
       " 1359: 'cried',\n",
       " 6543: 'wrote',\n",
       " 2888: 'immediately',\n",
       " 2720: 'hilarious',\n",
       " 4133: 'pandemi',\n",
       " 3600: 'media',\n",
       " 2776: 'honestly',\n",
       " 2126: 'feels',\n",
       " 6177: 'unforgivable',\n",
       " 4711: 'really',\n",
       " 4136: 'panic',\n",
       " 1157: 'coming',\n",
       " 2826: 'huh',\n",
       " 461: 'bat',\n",
       " 467: 'bats',\n",
       " 812: 'came',\n",
       " 5872: 'thing',\n",
       " 3819: 'naked',\n",
       " 2039: 'eye',\n",
       " 1594: 'die',\n",
       " 5026: 'scared',\n",
       " 6518: 'worried',\n",
       " 6520: 'worry',\n",
       " 1018: 'chlorox',\n",
       " 4005: 'okay',\n",
       " 4109: 'packages',\n",
       " 2349: 'fucking',\n",
       " 463: 'bathe',\n",
       " 767: 'bunch',\n",
       " 319: 'assholes',\n",
       " 170: 'amazon',\n",
       " 2072: 'fan',\n",
       " 3562: 'mask',\n",
       " 3564: 'masks',\n",
       " 3174: 'kept',\n",
       " 944: 'changing',\n",
       " 2099: 'fauci',\n",
       " 4556: 'protection',\n",
       " 4350: 'plastic',\n",
       " 407: 'bag',\n",
       " 3193: 'kill',\n",
       " 313: 'asphyxiate',\n",
       " 3442: 'lost',\n",
       " 1320: 'covid',\n",
       " 3816: 'nah',\n",
       " 713: 'bro',\n",
       " 3194: 'killed',\n",
       " 5523: 'stay',\n",
       " 2763: 'home',\n",
       " 2158: 'figured',\n",
       " 3744: 'monthses',\n",
       " 5690: 'supposed',\n",
       " 3115: 'job',\n",
       " 4201: 'pay',\n",
       " 4807: 'rent',\n",
       " 6448: 'whoa',\n",
       " 2750: 'hold',\n",
       " 1817: 'easy',\n",
       " 836: 'capitalism',\n",
       " 5105: 'send',\n",
       " 3736: 'money',\n",
       " 1325: 'crackhead',\n",
       " 5627: 'stuttering',\n",
       " 3443: 'lot',\n",
       " 6512: 'works',\n",
       " 153: 'allocate',\n",
       " 1598: 'different',\n",
       " 738: 'buckets',\n",
       " 5352: 'social',\n",
       " 5084: 'security',\n",
       " 3603: 'medicare',\n",
       " 3601: 'medicaid',\n",
       " 5694: 'sure',\n",
       " 4660: 'rainy',\n",
       " 1470: 'day',\n",
       " 2356: 'fund',\n",
       " 2488: 'gotta',\n",
       " 4443: 'pouring',\n",
       " 4085: 'outside',\n",
       " 608: 'blown',\n",
       " 2843: 'hurricane',\n",
       " 5073: 'season',\n",
       " 5129: 'seven',\n",
       " 2844: 'hurricanes',\n",
       " 2739: 'hitting',\n",
       " 1207: 'confession',\n",
       " 716: 'broke',\n",
       " 2119: 'federal',\n",
       " 2490: 'government',\n",
       " 1486: 'debt',\n",
       " 3688: 'minus',\n",
       " 3591: 'means',\n",
       " 3689: 'minuses',\n",
       " 5444: 'spit',\n",
       " 6057: 'trillion',\n",
       " 1693: 'dollars',\n",
       " 6452: 'whos',\n",
       " 417: 'balancing',\n",
       " 988: 'cheque',\n",
       " 639: 'book',\n",
       " 3583: 'mc',\n",
       " 2597: 'hammer',\n",
       " 3523: 'managing',\n",
       " 5254: 'situation',\n",
       " 3195: 'killing',\n",
       " 722: 'brown',\n",
       " 2010: 'expensive',\n",
       " 4090: 'overdraft',\n",
       " 2127: 'fees',\n",
       " 4309: 'piling',\n",
       " 3237: 'kyle',\n",
       " 5557: 'stock',\n",
       " 3549: 'market',\n",
       " 2054: 'fail',\n",
       " 2762: 'holy',\n",
       " 2674: 'heat',\n",
       " 4507: 'printer',\n",
       " 3507: 'make',\n",
       " 240: 'appear',\n",
       " 2360: 'funnel',\n",
       " 4404: 'ponzi',\n",
       " 5034: 'scheme',\n",
       " 1612: 'direct',\n",
       " 4756: 'reflection',\n",
       " 177: 'american',\n",
       " 2673: 'hearts',\n",
       " 293: 'arrows',\n",
       " 2464: 'going',\n",
       " 5396: 'souls',\n",
       " 178: 'americans',\n",
       " 6508: 'working',\n",
       " 5278: 'sky',\n",
       " 4914: 'rocketing',\n",
       " 6328: 'wait',\n",
       " 453: 'based',\n",
       " 4866: 'revenue',\n",
       " 5587: 'streams',\n",
       " 4629: 'quarterly',\n",
       " 1809: 'earnings',\n",
       " 3911: 'nobodys',\n",
       " 1960: 'everybody',\n",
       " 4059: 'ordering',\n",
       " 3091: 'jeff',\n",
       " 536: 'bezos',\n",
       " 5853: 'thank',\n",
       " 1961: 'everybodys',\n",
       " 5211: 'shut',\n",
       " 292: 'arrow',\n",
       " 2458: 'god',\n",
       " 3776: 'mouth',\n",
       " 3856: 'need',\n",
       " 2698: 'heres',\n",
       " 2851: 'hush',\n",
       " 5858: 'thatll',\n",
       " 6402: 'week',\n",
       " 255: 'appreciate',\n",
       " 5566: 'stop',\n",
       " 6084: 'trying',\n",
       " 1662: 'distract',\n",
       " 2765: 'homelessness',\n",
       " 157: 'alltime',\n",
       " 2708: 'high',\n",
       " 145: 'aliens',\n",
       " 994: 'chew',\n",
       " 3397: 'little',\n",
       " 574: 'bit',\n",
       " 5760: 'talk',\n",
       " 6557: 'yeah',\n",
       " 1342: 'crazy',\n",
       " 6175: 'unemployed',\n",
       " 3671: 'million',\n",
       " 3074: 'jada',\n",
       " 970: 'cheated',\n",
       " 5316: 'smith',\n",
       " 1914: 'entanglements',\n",
       " 2018: 'explain',\n",
       " 2807: 'hot',\n",
       " 5926: 'tip',\n",
       " 1393: 'cryptocurrency',\n",
       " 1194: 'computer',\n",
       " 3681: 'mining',\n",
       " 1602: 'digital',\n",
       " 1135: 'coins',\n",
       " 2061: 'fake',\n",
       " 1689: 'doing',\n",
       " 1217: 'confused',\n",
       " 2704: 'hey',\n",
       " 3888: 'nfts',\n",
       " 3887: 'nft',\n",
       " 601: 'blockchain',\n",
       " 4102: 'ownership',\n",
       " 2659: 'head',\n",
       " 1321: 'covids',\n",
       " 1467: 'dave',\n",
       " 951: 'chappelle',\n",
       " 2394: 'gave',\n",
       " 2738: 'hits',\n",
       " 504: 'beginning',\n",
       " 2702: 'hes',\n",
       " 6583: 'youve',\n",
       " 6377: 'watching',\n",
       " 1686: 'dog',\n",
       " 3335: 'let',\n",
       " 691: 'break',\n",
       " 717: 'broken',\n",
       " 1305: 'country',\n",
       " 5207: 'shows',\n",
       " 1278: 'cornfield',\n",
       " 3997: 'ohio',\n",
       " 1684: 'doesnt',\n",
       " 5397: 'sound',\n",
       " 5839: 'test',\n",
       " 1472: 'days',\n",
       " 4979: 'safe',\n",
       " 528: 'best',\n",
       " 2553: 'guaranteed',\n",
       " 1313: 'course',\n",
       " 5841: 'testing',\n",
       " 3482: 'machines',\n",
       " 6418: 'went',\n",
       " 3961: 'nuts',\n",
       " 3902: 'nigga',\n",
       " 4381: 'point',\n",
       " 4965: 'run',\n",
       " 4079: 'outbreaks',\n",
       " 1681: 'documentary',\n",
       " 2165: 'film',\n",
       " 2927: 'incredible',\n",
       " 5017: 'saw',\n",
       " 4656: 'radio',\n",
       " 1057: 'city',\n",
       " 3801: 'music',\n",
       " 2594: 'hall',\n",
       " 6374: 'watched',\n",
       " 1897: 'end',\n",
       " 3509: 'makes',\n",
       " 4959: 'ruined',\n",
       " 4172: 'party',\n",
       " 6339: 'walked',\n",
       " 6380: 'way',\n",
       " 1833: 'edited',\n",
       " 6296: 'villain',\n",
       " 6080: 'truth',\n",
       " 115: 'ah',\n",
       " 6343: 'walks',\n",
       " 375: 'away',\n",
       " 679: 'bradley',\n",
       " 1267: 'cooper',\n",
       " 2343: 'fucked',\n",
       " 6370: 'wasnt',\n",
       " 372: 'awardwinning',\n",
       " 3508: 'maker',\n",
       " 5020: 'saying',\n",
       " 3431: 'looks',\n",
       " 2100: 'fault',\n",
       " 5925: 'tiny',\n",
       " 3247: 'lady',\n",
       " 5304: 'small',\n",
       " 2609: 'handsy',\n",
       " 5978: 'touching',\n",
       " 2705: 'hi',\n",
       " 2062: 'fakecoughed',\n",
       " 3450: 'love',\n",
       " 1456: 'dark',\n",
       " 2835: 'humor',\n",
       " 2362: 'funny',\n",
       " 5840: 'tested',\n",
       " 4427: 'positive',\n",
       " 3273: 'later',\n",
       " 3220: 'knew',\n",
       " 3055: 'israeli',\n",
       " 2653: 'having',\n",
       " 3610: 'meeting',\n",
       " 2760: 'hollywood',\n",
       " 4127: 'palestinians',\n",
       " 2426: 'getting',\n",
       " 4446: 'powerful',\n",
       " 6150: 'unassuming',\n",
       " 2532: 'gremlin',\n",
       " 6082: 'try',\n",
       " 5709: 'survived',\n",
       " 809: 'calls',\n",
       " 4743: 'recovering',\n",
       " 357: 'austin',\n",
       " 3117: 'joe',\n",
       " 4920: 'rogan',\n",
       " 1708: 'dope',\n",
       " 3124: 'join',\n",
       " 6505: 'worked',\n",
       " 887: 'catch',\n",
       " 2744: 'ho',\n",
       " 1102: 'closet',\n",
       " 6576: 'youd',\n",
       " 5575: 'straight',\n",
       " 3127: 'joint',\n",
       " 3084: 'jayz',\n",
       " 6500: 'word',\n",
       " 3085: 'jayzs',\n",
       " 6400: 'weed',\n",
       " 1176: 'company',\n",
       " 5115: 'sent',\n",
       " 3128: 'joints',\n",
       " 6354: 'wanna',\n",
       " 4922: 'roll',\n",
       " 844: 'car',\n",
       " 5317: 'smoke',\n",
       " 1442: 'damn',\n",
       " 3974: 'obvious',\n",
       " 1386: 'cruising',\n",
       " 3369: 'lights',\n",
       " 5196: 'shouldve',\n",
       " 532: 'better',\n",
       " 5965: 'took',\n",
       " 2732: 'hit',\n",
       " 2906: 'impressions',\n",
       " 1675: 'djeah',\n",
       " 1309: 'couple',\n",
       " 1676: 'djeahhova',\n",
       " 3279: 'laughed',\n",
       " 2519: 'great',\n",
       " 4880: 'ride',\n",
       " 1768: 'dropped',\n",
       " 3691: 'minutes',\n",
       " 5508: 'started',\n",
       " 5849: 'texting',\n",
       " 6355: 'want',\n",
       " 5736: 'swoop',\n",
       " 5420: 'special',\n",
       " 2558: 'guests',\n",
       " 5232: 'silent',\n",
       " 6409: 'weird',\n",
       " 3983: 'odd',\n",
       " 2330: 'friends',\n",
       " 2652: 'havent',\n",
       " 2667: 'heard',\n",
       " 6405: 'weeks',\n",
       " 1468: 'daves',\n",
       " 2616: 'hanging',\n",
       " 3385: 'link',\n",
       " 5976: 'touched',\n",
       " 4036: 'opens',\n",
       " 5936: 'tmz',\n",
       " 4112: 'page',\n",
       " 5842: 'tests',\n",
       " 6072: 'true',\n",
       " 3762: 'motherfuckers',\n",
       " 807: 'called',\n",
       " 6054: 'tried',\n",
       " 1535: 'deny',\n",
       " 4567: 'prove',\n",
       " 5319: 'smoking',\n",
       " 5391: 'sorry',\n",
       " 6338: 'walk',\n",
       " 118: 'aids',\n",
       " 6341: 'walking',\n",
       " 6120: 'twice',\n",
       " 6513: 'world',\n",
       " 2489: 'gotten',\n",
       " 6119: 'twi',\n",
       " 763: 'bullshit',\n",
       " 6247: 'vaccinated',\n",
       " 4266: 'pfizer',\n",
       " 6248: 'vaccine',\n",
       " 2103: 'favorites',\n",
       " 2420: 'german',\n",
       " 5039: 'scientist',\n",
       " 3512: 'making',\n",
       " 2572: 'guy',\n",
       " 6555: 'yavi',\n",
       " 2817: 'hows',\n",
       " 6548: 'ya',\n",
       " 6545: 'wuhan',\n",
       " 117: 'ahead',\n",
       " 5033: 'schedule',\n",
       " 6556: 'yaza',\n",
       " 2743: 'hmm',\n",
       " 3066: 'iz',\n",
       " 4773: 'regular',\n",
       " 4528: 'products',\n",
       " 1153: 'comfort',\n",
       " 5812: 'tell',\n",
       " 5332: 'snake',\n",
       " 6271: 'venom',\n",
       " 70: 'added',\n",
       " 386: 'baby',\n",
       " 4444: 'powder',\n",
       " 5870: 'thicken',\n",
       " 4003: 'oil',\n",
       " 3434: 'loosen',\n",
       " 2459: 'goddamn',\n",
       " 2181: 'finished',\n",
       " 6458: 'wild',\n",
       " 6095: 'turkey',\n",
       " 4410: 'pop',\n",
       " 5192: 'shots',\n",
       " 4174: 'pass',\n",
       " 20: 'absurd',\n",
       " 6249: 'vaccines',\n",
       " 25: 'accents',\n",
       " 1: 'ab',\n",
       " 3722: 'moder',\n",
       " 3724: 'moderna',\n",
       " 1548: 'derna',\n",
       " 5190: 'shot',\n",
       " 1364: 'crip',\n",
       " 676: 'bra',\n",
       " 4437: 'potent',\n",
       " 5606: 'strong',\n",
       " 5417: 'speak',\n",
       " 3260: 'language',\n",
       " 4708: 'realize',\n",
       " 1909: 'english',\n",
       " 3881: 'newborn',\n",
       " 3879: 'new',\n",
       " 268: 'arabic',\n",
       " 186: 'ancient',\n",
       " 2545: 'grow',\n",
       " 3244: 'lacks',\n",
       " 1545: 'depth',\n",
       " 1986: 'exclamation',\n",
       " 2531: 'greetings',\n",
       " 5756: 'takes',\n",
       " 4207: 'peace',\n",
       " 3628: 'mercy',\n",
       " 1670: 'divine',\n",
       " 3366: 'light',\n",
       " 6214: 'unto',\n",
       " 4554: 'protect',\n",
       " 1923: 'entire',\n",
       " 3381: 'linea',\n",
       " 4107: 'pack',\n",
       " 1045: 'cigarettes',\n",
       " 5376: 'somebody',\n",
       " 1413: 'curses',\n",
       " 5241: 'sincerely',\n",
       " 1883: 'emotionally',\n",
       " 957: 'charged',\n",
       " 4012: 'oldie',\n",
       " 2476: 'goodie',\n",
       " 5825: 'tense',\n",
       " 3271: 'lately',\n",
       " 3581: 'maybe',\n",
       " 3730: 'mom',\n",
       " 2038: 'exwife',\n",
       " 1002: 'child',\n",
       " 4498: 'previous',\n",
       " 3552: 'marriage',\n",
       " 2556: 'guess',\n",
       " 2095: 'father',\n",
       " 551: 'biggest',\n",
       " 5799: 'technically',\n",
       " 318: 'asshole',\n",
       " 4620: 'pussy',\n",
       " 2965: 'insinuates',\n",
       " 6384: 'weakness',\n",
       " 3642: 'metaphorically',\n",
       " 4288: 'physically',\n",
       " 16: 'absolutely',\n",
       " 2827: 'human',\n",
       " 3361: 'life',\n",
       " 4442: 'pounds',\n",
       " 2211: 'flesh',\n",
       " 2753: 'hole',\n",
       " 4767: 'regenerates',\n",
       " 1974: 'exactly',\n",
       " 3277: 'laugh',\n",
       " 1295: 'cough',\n",
       " 4324: 'piss',\n",
       " 4703: 'ready',\n",
       " 2328: 'friend',\n",
       " 1030: 'chris',\n",
       " 4176: 'passed',\n",
       " 3191: 'kidney',\n",
       " 5561: 'stone',\n",
       " 1587: 'dick',\n",
       " 3847: 'near',\n",
       " 1595: 'died',\n",
       " 2623: 'happening',\n",
       " 5179: 'shoot',\n",
       " 2499: 'grain',\n",
       " 4994: 'sand',\n",
       " 6490: 'women',\n",
       " 1412: 'curse',\n",
       " 6502: 'words',\n",
       " 2514: 'graphic',\n",
       " 5507: 'start',\n",
       " 756: 'build',\n",
       " 3175: 'kes',\n",
       " 1870: 'emak',\n",
       " 3764: 'mothers',\n",
       " 5423: 'specific',\n",
       " 3342: 'level',\n",
       " 1864: 'eli',\n",
       " 3815: 'nafadek',\n",
       " 5942: 'toes',\n",
       " 1683: 'does',\n",
       " 3585: 'mea',\n",
       " 5296: 'slithered',\n",
       " 329: 'astaghfirullah',\n",
       " 2064: 'fall',\n",
       " 6309: 'visual',\n",
       " 4252: 'personal',\n",
       " 5774: 'tarekhek',\n",
       " 2730: 'history',\n",
       " 2524: 'greatgrandmother',\n",
       " 4484: 'present',\n",
       " 2366: 'future',\n",
       " 1677: 'dna',\n",
       " 6421: 'western',\n",
       " 2598: 'hand',\n",
       " 2423: 'gestures',\n",
       " 546: 'big',\n",
       " 1476: 'deal',\n",
       " 6141: 'uh',\n",
       " 1908: 'england',\n",
       " 4867: 'reverse',\n",
       " 5225: 'sign',\n",
       " 1218: 'confusing',\n",
       " 6359: 'war',\n",
       " 1264: 'cool',\n",
       " 269: 'arabs',\n",
       " 5222: 'sideways',\n",
       " 531: 'betizik',\n",
       " 6457: 'wiggle',\n",
       " 314: 'ass',\n",
       " 1219: 'confusion',\n",
       " 3314: 'leaving',\n",
       " 3646: 'mexicanos',\n",
       " 2546: 'growing',\n",
       " 2533: 'grew',\n",
       " 4115: 'pain',\n",
       " 2613: 'hang',\n",
       " 4840: 'resilient',\n",
       " 1258: 'conversation',\n",
       " 4623: 'putting',\n",
       " 6344: 'wall',\n",
       " 4813: 'repeating',\n",
       " 3647: 'mexicans',\n",
       " 1347: 'creative',\n",
       " 5758: 'taking',\n",
       " 3937: 'notes',\n",
       " 1853: 'el',\n",
       " 949: 'chapo',\n",
       " 5347: 'snuck',\n",
       " 6091: 'tunnel',\n",
       " 3769: 'motorcycle',\n",
       " 3: 'abdallah',\n",
       " 6373: 'watch',\n",
       " 2579: 'habibi',\n",
       " 148: 'allah',\n",
       " 5867: 'theyll',\n",
       " 6590: 'zipline',\n",
       " 3136: 'josé',\n",
       " 4615: 'push',\n",
       " 3760: 'mother',\n",
       " 5866: 'theyd',\n",
       " 3700: 'missed',\n",
       " 6007: 'trampoline',\n",
       " 6591: 'ziplining',\n",
       " 6053: 'tricky',\n",
       " 5921: 'timing',\n",
       " 3644: 'mexican',\n",
       " 3402: 'living',\n",
       " 3648: 'mexico',\n",
       " 2856: 'id',\n",
       " 2219: 'flip',\n",
       " 5061: 'script',\n",
       " 757: 'building',\n",
       " 821: 'cancùn',\n",
       " 4621: 'puto',\n",
       " 6444: 'white',\n",
       " 2438: 'girls',\n",
       " 1391: 'crying',\n",
       " 176: 'america',\n",
       " 5463: 'spring',\n",
       " 4653: 'racist',\n",
       " 6169: 'understand',\n",
       " 4588: 'puerto',\n",
       " 6257: 'vallarta',\n",
       " 2355: 'fun',\n",
       " 2421: 'germans',\n",
       " 4028: 'ooh',\n",
       " 3452: 'lovely',\n",
       " 4969: 'russians',\n",
       " 222: 'anybody',\n",
       " 6078: 'trust',\n",
       " 4968: 'russian',\n",
       " 77: 'adidas',\n",
       " 5991: 'tracksuit',\n",
       " 2472: 'goo',\n",
       " 24: 'accent',\n",
       " 904: 'cauliflower',\n",
       " 1810: 'ears',\n",
       " 3712: 'mma',\n",
       " 3182: 'khabib',\n",
       " 3957: 'nurmagomedov',\n",
       " 2521: 'greatest',\n",
       " 2154: 'fighter',\n",
       " 5764: 'talks',\n",
       " 5836: 'terrifying',\n",
       " 5590: 'streets',\n",
       " 3774: 'mountain',\n",
       " 2688: 'hell',\n",
       " 2153: 'fight',\n",
       " 941: 'change',\n",
       " 4435: 'potato',\n",
       " 6535: 'wrestled',\n",
       " 482: 'bears',\n",
       " 1850: 'eightyearold',\n",
       " 1435: 'dagestanis',\n",
       " 3343: 'levels',\n",
       " 971: 'chechnyan',\n",
       " 6534: 'wrestle',\n",
       " 237: 'apparently',\n",
       " 1003: 'childhood',\n",
       " 4850: 'rest',\n",
       " 5395: 'soul',\n",
       " 1116: 'coaches',\n",
       " 2621: 'happen',\n",
       " 1432: 'dad',\n",
       " 1433: 'daddy',\n",
       " 6000: 'train',\n",
       " 5383: 'son',\n",
       " 480: 'bear',\n",
       " 721: 'brought',\n",
       " 2812: 'house',\n",
       " 2779: 'honey',\n",
       " 4785: 'relax',\n",
       " 705: 'bring',\n",
       " 1504: 'defeat',\n",
       " 3330: 'leprechaun',\n",
       " 5013: 'save',\n",
       " 3050: 'islam',\n",
       " 141: 'alhamdulillah',\n",
       " 6494: 'wonderful',\n",
       " 3637: 'messaged',\n",
       " 2093: 'fat',\n",
       " 893: 'caterpillar',\n",
       " 755: 'bugs',\n",
       " 6472: 'wings',\n",
       " 487: 'beautiful',\n",
       " 790: 'butterfly',\n",
       " 594: 'blessing',\n",
       " 3486: 'mad',\n",
       " 2109: 'fear',\n",
       " 1345: 'created',\n",
       " 1671: 'division',\n",
       " 2946: 'infuriating',\n",
       " 6316: 'voting',\n",
       " 4744: 'red',\n",
       " 610: 'blue',\n",
       " 604: 'bloods',\n",
       " 1366: 'crips',\n",
       " 2378: 'gangsters',\n",
       " 3865: 'neighbor',\n",
       " 4747: 'redneck',\n",
       " 4481: 'prepared',\n",
       " 2410: 'generators',\n",
       " 2787: 'hooked',\n",
       " 4367: 'plugged',\n",
       " 2032: 'extension',\n",
       " 1271: 'cord',\n",
       " 1395: 'culdesac',\n",
       " 5118: 'separate',\n",
       " 3726: 'mohammeds',\n",
       " 5051: 'scotts',\n",
       " 4166: 'particular',\n",
       " 106: 'agenda',\n",
       " 512: 'believe',\n",
       " 6033: 'travel',\n",
       " 129: 'airport',\n",
       " 2300: 'fragmented',\n",
       " 5644: 'suck',\n",
       " 4648: 'race',\n",
       " 736: 'bubbles',\n",
       " 4202: 'paying',\n",
       " 345: 'attention',\n",
       " 6577: 'youll',\n",
       " 5649: 'sudden',\n",
       " 5568: 'stops',\n",
       " 5890: 'thousands',\n",
       " 6542: 'wrong',\n",
       " 6036: 'travelling',\n",
       " 870: 'carryon',\n",
       " 6435: 'wheels',\n",
       " 5193: 'shoulder',\n",
       " 3635: 'mess',\n",
       " 5291: 'sliding',\n",
       " 1998: 'exist',\n",
       " 5471: 'square',\n",
       " 2262: 'footage',\n",
       " 5306: 'smart',\n",
       " 2991: 'intelligent',\n",
       " 2287: 'forward',\n",
       " 400: 'backwards',\n",
       " 6451: 'whore',\n",
       " 3147: 'juke',\n",
       " 6431: 'wheel',\n",
       " 4699: 'read',\n",
       " 3954: 'numbers',\n",
       " 616: 'boarding',\n",
       " 2388: 'gate',\n",
       " 2542: 'group',\n",
       " 5130: 'sevens',\n",
       " 4340: 'plane',\n",
       " 1152: 'comes',\n",
       " 411: 'bags',\n",
       " 1105: 'clotheslining',\n",
       " 1113: 'clue',\n",
       " 1732: 'dragging',\n",
       " 5617: 'stuck',\n",
       " 6097: 'turn',\n",
       " 3525: 'maneuver',\n",
       " 1762: 'drives',\n",
       " 2958: 'insane',\n",
       " 2217: 'flight',\n",
       " 344: 'attendants',\n",
       " 4843: 'respect',\n",
       " 2180: 'finish',\n",
       " 5015: 'saving',\n",
       " 5121: 'seriously',\n",
       " 3056: 'issue',\n",
       " 1679: 'doctor',\n",
       " 760: 'bulkhead',\n",
       " 2228: 'floor',\n",
       " 5186: 'shortcircuit',\n",
       " 5754: 'takeoff',\n",
       " 877: 'case',\n",
       " 1877: 'emergency',\n",
       " 2002: 'exit',\n",
       " 3667: 'miles',\n",
       " 2810: 'hour',\n",
       " 37: 'according',\n",
       " 2045: 'faaaaa',\n",
       " 4981: 'safety',\n",
       " 201: 'announcements',\n",
       " 5075: 'seatbelt',\n",
       " 2128: 'feet',\n",
       " 4594: 'pummel',\n",
       " 2890: 'imminent',\n",
       " 1480: 'death',\n",
       " 2224: 'floating',\n",
       " 5410: 'space',\n",
       " 1335: 'crashes',\n",
       " 2728: 'hire',\n",
       " 5878: 'thirdparty',\n",
       " 1333: 'crash',\n",
       " 68: 'adams',\n",
       " 6585: 'yup',\n",
       " 972: 'check',\n",
       " 5243: 'single',\n",
       " 4923: 'roller',\n",
       " 1120: 'coaster',\n",
       " 4666: 'ralph',\n",
       " 343: 'attendant',\n",
       " 278: 'argue',\n",
       " 3255: 'landing',\n",
       " 403: 'bad',\n",
       " 6093: 'turbulence',\n",
       " 2240: 'fly',\n",
       " 3853: 'neck',\n",
       " 5057: 'screaming',\n",
       " 3401: 'lives',\n",
       " 5303: 'smacking',\n",
       " 4797: 'remember',\n",
       " 3436: 'lose',\n",
       " 798: 'cabin',\n",
       " 4490: 'pressure',\n",
       " 4104: 'oxygen',\n",
       " 4962: 'rule',\n",
       " 2213: 'flew',\n",
       " 6575: 'york',\n",
       " 622: 'body',\n",
       " 6378: 'water',\n",
       " 5224: 'sight',\n",
       " 2231: 'flotation',\n",
       " 1573: 'device',\n",
       " 740: 'buckle',\n",
       " 4589: 'pull',\n",
       " 5598: 'string',\n",
       " 2939: 'inflate',\n",
       " 5583: 'straw',\n",
       " 4830: 'requires',\n",
       " 695: 'breath',\n",
       " 3493: 'magical',\n",
       " 3538: 'manually',\n",
       " 3869: 'nephews',\n",
       " 4405: 'pool',\n",
       " 2223: 'floaties',\n",
       " 3657: 'middle',\n",
       " 3290: 'layer',\n",
       " 2215: 'flickering',\n",
       " 4757: 'reflective',\n",
       " 5261: 'size',\n",
       " 1603: 'dime',\n",
       " 6442: 'whistle',\n",
       " 6263: 'vast',\n",
       " 1827: 'echoy',\n",
       " 3981: 'oceans',\n",
       " 1811: 'earth',\n",
       " 3514: 'malaysia',\n",
       " 3150: 'jumbo',\n",
       " 3104: 'jet',\n",
       " 5947: 'told',\n",
       " 4466: 'predators',\n",
       " 1973: 'exact',\n",
       " 3409: 'location',\n",
       " 2135: 'felt',\n",
       " 4763: 'refugee',\n",
       " 1055: 'citizen',\n",
       " 806: 'california',\n",
       " 1951: 'estate',\n",
       " 266: 'arabamerican',\n",
       " 4126: 'palestinianarabamer',\n",
       " 247: 'applause',\n",
       " 2861: 'identify',\n",
       " 6196: 'unless',\n",
       " 795: 'buying',\n",
       " 2566: 'gun',\n",
       " 4632: 'question',\n",
       " 2729: 'hispanic',\n",
       " 5677: 'super',\n",
       " 101: 'africanameric',\n",
       " 5939: 'today',\n",
       " 1713: 'double',\n",
       " 446: 'barrel',\n",
       " 2393: 'gauge',\n",
       " 5919: 'times',\n",
       " 3836: 'natives',\n",
       " 3835: 'native',\n",
       " 100: 'african',\n",
       " 582: 'black',\n",
       " 99: 'africa',\n",
       " 3920: 'nonetheless',\n",
       " 3645: 'mexicanamericans',\n",
       " 304: 'asianamericans',\n",
       " 2283: 'forth',\n",
       " 5466: 'sprouted',\n",
       " 1275: 'corn',\n",
       " 2150: 'fields',\n",
       " 5461: 'spread',\n",
       " 3254: 'land',\n",
       " 4983: 'saint',\n",
       " 4111: 'paddys',\n",
       " 2522: 'greatgranddaddy',\n",
       " 4628: 'quarter',\n",
       " 3043: 'irish',\n",
       " 2090: 'fascinating',\n",
       " 1399: 'culture',\n",
       " 5462: 'spreading',\n",
       " 6420: 'west',\n",
       " 4495: 'pretty',\n",
       " 2786: 'hookah',\n",
       " 1398: 'culturally',\n",
       " 2868: 'ignorant',\n",
       " 1533: 'denise',\n",
       " 633: 'bong',\n",
       " 4069: 'ornate',\n",
       " 5938: 'tobaccos',\n",
       " 2208: 'flavors',\n",
       " 4319: 'pipe',\n",
       " 4573: 'prrr',\n",
       " 606: 'blow',\n",
       " 4320: 'pipes',\n",
       " 723: 'brrr',\n",
       " 1661: 'distilled',\n",
       " 470: 'battery',\n",
       " 875: 'cartridge',\n",
       " 3808: 'mysterious',\n",
       " 3388: 'liquid',\n",
       " 4633: 'questioning',\n",
       " 4238: 'perfectly',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"pickle\\cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.009*\"audience\" + 0.008*\"fuck\" + 0.007*\"said\" + 0.006*\"going\" + 0.005*\"did\" + 0.005*\"want\" + 0.005*\"way\" + 0.004*\"chuckling\" + 0.004*\"didnt\" + 0.004*\"laughing\" \n",
      "\n",
      "Topic 1 \n",
      " 0.007*\"yeah\" + 0.005*\"good\" + 0.004*\"really\" + 0.004*\"make\" + 0.004*\"want\" + 0.004*\"didnt\" + 0.004*\"wanna\" + 0.004*\"did\" + 0.004*\"thing\" + 0.004*\"say\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.007*\"said\" + 0.007*\"fuck\" + 0.006*\"did\" + 0.005*\"man\" + 0.005*\"trans\" + 0.005*\"want\" + 0.005*\"say\" + 0.004*\"white\" + 0.004*\"going\" + 0.004*\"good\" \n",
      "\n",
      "Topic 1 \n",
      " 0.019*\"audience\" + 0.010*\"chuckling\" + 0.008*\"laughing\" + 0.006*\"wanna\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.004*\"love\" + 0.004*\"fuck\" + 0.004*\"feel\" + 0.003*\"going\" \n",
      "\n",
      "Topic 2 \n",
      " 0.009*\"yeah\" + 0.006*\"really\" + 0.006*\"want\" + 0.005*\"way\" + 0.005*\"thing\" + 0.005*\"going\" + 0.005*\"good\" + 0.005*\"make\" + 0.004*\"said\" + 0.004*\"thought\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.007*\"said\" + 0.007*\"yeah\" + 0.006*\"fuck\" + 0.006*\"going\" + 0.005*\"did\" + 0.005*\"way\" + 0.005*\"really\" + 0.005*\"say\" + 0.005*\"want\" + 0.005*\"didnt\" \n",
      "\n",
      "Topic 1 \n",
      " 0.020*\"audience\" + 0.010*\"chuckling\" + 0.009*\"laughing\" + 0.006*\"want\" + 0.005*\"yeah\" + 0.005*\"love\" + 0.004*\"didnt\" + 0.004*\"lot\" + 0.004*\"make\" + 0.004*\"good\" \n",
      "\n",
      "Topic 2 \n",
      " 0.005*\"ngga\" + 0.005*\"police\" + 0.005*\"did\" + 0.005*\"black\" + 0.005*\"said\" + 0.005*\"going\" + 0.004*\"murdered\" + 0.004*\"man\" + 0.004*\"guy\" + 0.003*\"fuck\" \n",
      "\n",
      "Topic 3 \n",
      " 0.007*\"good\" + 0.006*\"uh\" + 0.006*\"old\" + 0.006*\"chinese\" + 0.005*\"asian\" + 0.004*\"wang\" + 0.004*\"ive\" + 0.004*\"want\" + 0.004*\"eyes\" + 0.004*\"cork\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "for topic, topwords in lda.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>ladies and gentleman  you can feel excitemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>june    is a performance special by comedian d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>listen carefully   this is for my favori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>this is the latest ive stayed up in a long tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>emotional music playing music ends drew michae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>thank you thank you oh my gosh thank you so mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>whoo    kathleen   madigan   kathleen   madi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>recorded at the madison square garden on augus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>all right  wow so nice oh wow gosh thats wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>aired december    please welcome to the stage ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Transcript\n",
       "Amer_Mohammed     ladies and gentleman  you can feel excitemen...\n",
       "Dave_2          june    is a performance special by comedian d...\n",
       "Dave_Chappelle        listen carefully   this is for my favori...\n",
       "Drew_Michael_1  this is the latest ive stayed up in a long tim...\n",
       "Drew_Michael_2  emotional music playing music ends drew michae...\n",
       "Jim_Gaffigan    thank you thank you oh my gosh thank you so mu...\n",
       "Kathleen          whoo    kathleen   madigan   kathleen   madi...\n",
       "Lousi           recorded at the madison square garden on augus...\n",
       "Phil               all right  wow so nice oh wow gosh thats wa...\n",
       "Tom             aired december    please welcome to the stage ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('pickle\\data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>ladies gentleman excitement air texas show roa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>june performance dave chappelle violence ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>band beings faithful evidence things book mans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>time mm yeah i i people youre people sort cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>music music people time people audience chuckl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>thank thank thank thank thats gon week im itll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>whoo kathleen madigan shes stage gon pants cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>madison garden august time bums dime didnt peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>wow thats way keep palladium thank days stage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>december please stage thank cleveland ohio tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Transcript\n",
       "Amer_Mohammed   ladies gentleman excitement air texas show roa...\n",
       "Dave_2          june performance dave chappelle violence ameri...\n",
       "Dave_Chappelle  band beings faithful evidence things book mans...\n",
       "Drew_Michael_1  time mm yeah i i people youre people sort cros...\n",
       "Drew_Michael_2  music music people time people audience chuckl...\n",
       "Jim_Gaffigan    thank thank thank thank thats gon week im itll...\n",
       "Kathleen        whoo kathleen madigan shes stage gon pants cau...\n",
       "Lousi           madison garden august time bums dime didnt peo...\n",
       "Phil            wow thats way keep palladium thank days stage ...\n",
       "Tom             december please stage thank cleveland ohio tha..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean['Transcript'].apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abs</th>\n",
       "      <th>absence</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accent</th>\n",
       "      <th>accents</th>\n",
       "      <th>access</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 3959 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ability  abortion  abraham  abs  absence  abuse  abyss  \\\n",
       "Amer_Mohammed         0         0        0    0        0      0      0   \n",
       "Dave_2                0         0        0    0        0      0      0   \n",
       "Dave_Chappelle        0         0        0    0        0      1      0   \n",
       "Drew_Michael_1        2         0        0    1        0      0      0   \n",
       "Drew_Michael_2        1         1        0    0        1      0      0   \n",
       "Jim_Gaffigan          0         0        0    0        0      0      0   \n",
       "Kathleen              0         0        1    0        0      0      0   \n",
       "Lousi                 0         0        0    0        0      0      0   \n",
       "Phil                  0         0        0    0        0      0      1   \n",
       "Tom                   0         0        0    0        0      0      0   \n",
       "\n",
       "                accent  accents  access  ...  youll  youth  youtube  youve  \\\n",
       "Amer_Mohammed        2        2       0  ...      0      0        0      1   \n",
       "Dave_2               0        0       0  ...      0      0        1      0   \n",
       "Dave_Chappelle       0        0       0  ...      0      0        1      0   \n",
       "Drew_Michael_1       0        0       0  ...      0      0        0      2   \n",
       "Drew_Michael_2       0        0       2  ...      1      0        0      4   \n",
       "Jim_Gaffigan         0        0       0  ...      1      1        0      1   \n",
       "Kathleen             0        0       0  ...      3      0        0      4   \n",
       "Lousi                1        0       0  ...      0      0        2      1   \n",
       "Phil                11        3       0  ...      1      1        0      2   \n",
       "Tom                  0        0       0  ...      2      0        1      2   \n",
       "\n",
       "                zimbabwe  zimmerman  zip  ziplining  zone  zoo  \n",
       "Amer_Mohammed          0          0    0          1     0    0  \n",
       "Dave_2                 0          6    0          0     0    0  \n",
       "Dave_Chappelle         0          0    0          0     0    0  \n",
       "Drew_Michael_1         0          0    0          0     0    0  \n",
       "Drew_Michael_2         0          0    0          0     0    0  \n",
       "Jim_Gaffigan           0          0    0          0     0    0  \n",
       "Kathleen               0          0    1          0     0    0  \n",
       "Lousi                  1          0    0          0     0    9  \n",
       "Phil                   0          0    0          0     0    0  \n",
       "Tom                    0          0    0          0     1    0  \n",
       "\n",
       "[10 rows x 3959 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like','im','just','know','dont','right','people','thats','youre','oh','got','time','look','think', 'gonna', 'shit', 'cause', 'theres', 'theyre']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cv_noun = CountVectorizer(stop_words=stop_words)\n",
    "data_cv_noun = cv_noun.fit_transform(data_nouns['Transcript'])\n",
    "data_dtm_noun = pd.DataFrame(data_cv_noun.toarray(), columns=cv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = data_nouns.index\n",
    "data_dtm_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_noun = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_noun.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_noun = dict((v, k) for k, v in cv_noun.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.017*\"audience\" + 0.009*\"way\" + 0.008*\"thing\" + 0.008*\"fuck\" + 0.007*\"day\" + 0.007*\"lot\" + 0.006*\"man\" + 0.006*\"guy\" + 0.006*\"life\" + 0.006*\"things\" \n",
      "\n",
      "Topic 1 \n",
      " 0.009*\"gon\" + 0.007*\"way\" + 0.007*\"woman\" + 0.007*\"man\" + 0.006*\"yeah\" + 0.006*\"person\" + 0.006*\"trans\" + 0.006*\"thing\" + 0.005*\"guy\" + 0.005*\"hes\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "lda_noun = models.LdaModel(corpus=corpus_noun, num_topics=2, id2word=id2word_noun, passes=10)\n",
    "for topic, topwords in lda_noun.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.021*\"audience\" + 0.010*\"way\" + 0.008*\"woman\" + 0.007*\"thing\" + 0.007*\"man\" + 0.007*\"lot\" + 0.007*\"guy\" + 0.007*\"fuck\" + 0.006*\"trans\" + 0.006*\"things\" \n",
      "\n",
      "Topic 1 \n",
      " 0.013*\"fuck\" + 0.012*\"yeah\" + 0.009*\"thing\" + 0.009*\"bro\" + 0.009*\"way\" + 0.007*\"man\" + 0.007*\"gon\" + 0.007*\"okay\" + 0.007*\"lot\" + 0.006*\"hummus\" \n",
      "\n",
      "Topic 2 \n",
      " 0.011*\"gon\" + 0.007*\"day\" + 0.006*\"way\" + 0.006*\"guy\" + 0.005*\"kids\" + 0.005*\"man\" + 0.005*\"thing\" + 0.004*\"children\" + 0.004*\"yeah\" + 0.004*\"kind\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "lda_noun = models.LdaModel(corpus=corpus_noun, num_topics=3, id2word=id2word_noun, passes=10)\n",
    "for topic, topwords in lda_noun.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.007*\"guy\" + 0.006*\"gon\" + 0.006*\"way\" + 0.006*\"day\" + 0.006*\"men\" + 0.006*\"body\" + 0.005*\"kids\" + 0.005*\"man\" + 0.005*\"wife\" + 0.005*\"person\" \n",
      "\n",
      "Topic 1 \n",
      " 0.013*\"trans\" + 0.013*\"woman\" + 0.010*\"man\" + 0.008*\"chappelle\" + 0.007*\"bitch\" + 0.007*\"community\" + 0.007*\"way\" + 0.007*\"person\" + 0.007*\"comedy\" + 0.006*\"women\" \n",
      "\n",
      "Topic 2 \n",
      " 0.025*\"audience\" + 0.013*\"fuck\" + 0.013*\"way\" + 0.011*\"thing\" + 0.010*\"yeah\" + 0.009*\"lot\" + 0.007*\"hes\" + 0.007*\"person\" + 0.007*\"life\" + 0.006*\"day\" \n",
      "\n",
      "Topic 3 \n",
      " 0.013*\"gon\" + 0.008*\"day\" + 0.007*\"kids\" + 0.006*\"thing\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"lot\" + 0.006*\"life\" + 0.005*\"car\" + 0.005*\"guy\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "lda_noun = models.LdaModel(corpus=corpus_noun, num_topics=4, id2word=id2word_noun, passes=10)\n",
    "for topic, topwords in lda_noun.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>ladies gentleman excitement air houston texas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>june performance special comedian dave chappel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>favorite band human beings faithful graceful t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>latest ive long time mm cranky yeah i uh i peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>emotional music music drew nice people hard ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>thank gosh thank much thank much thank thats n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>whoo kathleen madigan madigan shes stage gon p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>madison square garden august time fine bums di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>right wow nice wow gosh thats way nice keep lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>aired december please stage tom papa thank gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Transcript\n",
       "Amer_Mohammed   ladies gentleman excitement air houston texas ...\n",
       "Dave_2          june performance special comedian dave chappel...\n",
       "Dave_Chappelle  favorite band human beings faithful graceful t...\n",
       "Drew_Michael_1  latest ive long time mm cranky yeah i uh i peo...\n",
       "Drew_Michael_2  emotional music music drew nice people hard ti...\n",
       "Jim_Gaffigan    thank gosh thank much thank much thank thats n...\n",
       "Kathleen        whoo kathleen madigan madigan shes stage gon p...\n",
       "Lousi           madison square garden august time fine bums di...\n",
       "Phil            right wow nice wow gosh thats way nice keep lo...\n",
       "Tom             aired december please stage tom papa thank gre..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_noun_adj = pd.DataFrame(data_clean['Transcript'].apply(nouns_adj))\n",
    "data_noun_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abs</th>\n",
       "      <th>absence</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4840 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ab  abandoned  ability  able  abolish  abortion  abraham  abs  \\\n",
       "Amer_Mohammed    1          0        0     1        0         0        0    0   \n",
       "Dave_2           0          0        0     0        0         0        0    0   \n",
       "Dave_Chappelle   0          0        0     3        0         0        0    0   \n",
       "Drew_Michael_1   0          0        2     2        0         0        0    3   \n",
       "Drew_Michael_2   0          0        1     5        0         1        0    0   \n",
       "Jim_Gaffigan     0          0        0     3        0         0        0    0   \n",
       "Kathleen         0          0        0     1        0         0        1    0   \n",
       "Lousi            0          0        0     1        1         0        0    0   \n",
       "Phil             0          1        0     0        0         0        0    0   \n",
       "Tom              0          0        0     0        0         0        0    0   \n",
       "\n",
       "                absence  abstract  ...  youngest  youth  youtube  youve  \\\n",
       "Amer_Mohammed         0         0  ...         0      0        0      1   \n",
       "Dave_2                0         0  ...         0      0        1      0   \n",
       "Dave_Chappelle        0         0  ...         0      0        1      0   \n",
       "Drew_Michael_1        0         0  ...         0      0        0      3   \n",
       "Drew_Michael_2        1         0  ...         0      0        0      4   \n",
       "Jim_Gaffigan          0         0  ...         1      1        0      3   \n",
       "Kathleen              0         0  ...         4      0        0      4   \n",
       "Lousi                 0         0  ...         0      0        2      1   \n",
       "Phil                  0         6  ...         0      1        0      5   \n",
       "Tom                   0         1  ...         0      0        1      2   \n",
       "\n",
       "                zimbabwe  zimmerman  zip  ziplining  zone  zoo  \n",
       "Amer_Mohammed          0          0    0          1     0    0  \n",
       "Dave_2                 0          6    0          0     0    0  \n",
       "Dave_Chappelle         0          0    0          0     0    0  \n",
       "Drew_Michael_1         0          0    0          0     0    0  \n",
       "Drew_Michael_2         0          0    0          0     0    0  \n",
       "Jim_Gaffigan           0          0    0          0     0    0  \n",
       "Kathleen               0          0    1          0     0    0  \n",
       "Lousi                  1          0    0          0     0    9  \n",
       "Phil                   0          0    0          0     0    0  \n",
       "Tom                    0          0    0          0     1    0  \n",
       "\n",
       "[10 rows x 4840 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cv_noun_adj = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cv_noun_adj = cv_noun_adj.fit_transform(data_noun_adj['Transcript'])\n",
    "data_dtm_noun_adj = pd.DataFrame(data_cv_noun_adj.toarray(), columns=cv_noun_adj.get_feature_names_out())\n",
    "data_dtm_noun_adj.index = data_noun_adj.index\n",
    "data_dtm_noun_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpus_noun_adj = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtm_noun_adj.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2word_noun_adj = dict((v, k) for k, v in cv_noun_adj.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.018*\"audience\" + 0.007*\"fuck\" + 0.006*\"trans\" + 0.006*\"black\" + 0.004*\"everybody\" + 0.004*\"gay\" + 0.004*\"chappelle\" + 0.003*\"bitch\" + 0.003*\"community\" + 0.003*\"somebody\" \n",
      "\n",
      "Topic 1 \n",
      " 0.006*\"fuck\" + 0.004*\"okay\" + 0.004*\"bro\" + 0.004*\"eyes\" + 0.003*\"body\" + 0.003*\"wife\" + 0.003*\"hummus\" + 0.003*\"baby\" + 0.003*\"face\" + 0.003*\"children\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "lda_noun_adj = models.LdaModel(corpus=corpus_noun_adj, num_topics=2, id2word=id2word_noun_adj, passes=10)\n",
    "for topic, topwords in lda_noun_adj.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.005*\"chinese\" + 0.005*\"fuck\" + 0.004*\"mom\" + 0.004*\"eyes\" + 0.004*\"body\" + 0.004*\"okay\" + 0.003*\"baby\" + 0.003*\"somebody\" + 0.003*\"fuckin\" + 0.003*\"problem\" \n",
      "\n",
      "Topic 1 \n",
      " 0.033*\"audience\" + 0.015*\"fuck\" + 0.008*\"bro\" + 0.006*\"okay\" + 0.006*\"hummus\" + 0.005*\"everybody\" + 0.003*\"baby\" + 0.003*\"ass\" + 0.003*\"prayer\" + 0.003*\"bitch\" \n",
      "\n",
      "Topic 2 \n",
      " 0.008*\"trans\" + 0.007*\"black\" + 0.005*\"wife\" + 0.005*\"chappelle\" + 0.004*\"community\" + 0.004*\"bitch\" + 0.004*\"children\" + 0.004*\"family\" + 0.004*\"ngga\" + 0.004*\"gay\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "lda_noun_adj = models.LdaModel(corpus=corpus_noun_adj, num_topics=3, id2word=id2word_noun_adj, passes=10)\n",
    "for topic, topwords in lda_noun_adj.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.059*\"audience\" + 0.010*\"fuck\" + 0.005*\"love\" + 0.004*\"na\" + 0.004*\"everybody\" + 0.004*\"twitter\" + 0.004*\"emotional\" + 0.003*\"thoughts\" + 0.003*\"chuckles\" + 0.003*\"yelp\" \n",
      "\n",
      "Topic 1 \n",
      " 0.005*\"mom\" + 0.005*\"fuck\" + 0.004*\"okay\" + 0.004*\"face\" + 0.004*\"children\" + 0.004*\"baby\" + 0.004*\"somebody\" + 0.003*\"car\" + 0.003*\"house\" + 0.003*\"na\" \n",
      "\n",
      "Topic 2 \n",
      " 0.011*\"trans\" + 0.007*\"black\" + 0.006*\"chappelle\" + 0.005*\"community\" + 0.005*\"gay\" + 0.005*\"bitch\" + 0.004*\"everybody\" + 0.004*\"family\" + 0.004*\"night\" + 0.003*\"audience\" \n",
      "\n",
      "Topic 3 \n",
      " 0.011*\"fuck\" + 0.008*\"bro\" + 0.006*\"okay\" + 0.006*\"hummus\" + 0.005*\"chinese\" + 0.004*\"asian\" + 0.004*\"baby\" + 0.004*\"black\" + 0.004*\"eyes\" + 0.004*\"bitch\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "lda_noun_adj = models.LdaModel(corpus=corpus_noun_adj, num_topics=4, id2word=id2word_noun_adj, passes=10)\n",
    "for topic, topwords in lda_noun_adj.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.017*\"fuck\" + 0.014*\"bro\" + 0.011*\"okay\" + 0.011*\"hummus\" + 0.005*\"baby\" + 0.005*\"ass\" + 0.005*\"prayer\" + 0.005*\"everybody\" + 0.005*\"bitch\" + 0.003*\"father\" \n",
      "\n",
      "Topic 1 \n",
      " 0.011*\"trans\" + 0.010*\"black\" + 0.007*\"fuck\" + 0.007*\"gay\" + 0.006*\"chappelle\" + 0.006*\"bitch\" + 0.005*\"community\" + 0.005*\"everybody\" + 0.005*\"ngga\" + 0.004*\"police\" \n",
      "\n",
      "Topic 2 \n",
      " 0.006*\"children\" + 0.005*\"wife\" + 0.004*\"car\" + 0.004*\"house\" + 0.004*\"family\" + 0.004*\"dad\" + 0.004*\"night\" + 0.003*\"mom\" + 0.003*\"ass\" + 0.003*\"news\" \n",
      "\n",
      "Topic 3 \n",
      " 0.030*\"audience\" + 0.007*\"fuck\" + 0.005*\"body\" + 0.005*\"eyes\" + 0.005*\"chinese\" + 0.004*\"na\" + 0.004*\"asian\" + 0.004*\"wan\" + 0.003*\"ta\" + 0.003*\"cork\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "lda_noun_adj = models.LdaModel(corpus=corpus_noun_adj, num_topics=4, id2word=id2word_noun_adj, passes=80)\n",
    "for topic, topwords in lda_noun_adj.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: hummus, prayer\n",
    "* Topic 1: trans, black\n",
    "* Topic 2: family\n",
    "* Topic 3: asian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Amer_Mohammed'),\n",
       " (1, 'Dave_2'),\n",
       " (1, 'Dave_Chappelle'),\n",
       " (3, 'Drew_Michael_1'),\n",
       " (3, 'Drew_Michael_2'),\n",
       " (2, 'Jim_Gaffigan'),\n",
       " (2, 'Kathleen'),\n",
       " (1, 'Lousi'),\n",
       " (3, 'Phil'),\n",
       " (2, 'Tom')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = lda_noun_adj[corpus_noun_adj]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtm_noun_adj.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.005*\"family\" + 0.004*\"ass\" + 0.004*\"house\" + 0.004*\"wife\" + 0.004*\"morning\" + 0.004*\"night\" + 0.004*\"children\" + 0.003*\"card\" + 0.003*\"stuff\" + 0.003*\"summer\" \n",
      "\n",
      "Topic 1 \n",
      " 0.006*\"trans\" + 0.005*\"black\" + 0.004*\"gay\" + 0.004*\"chinese\" + 0.004*\"fuck\" + 0.003*\"id\" + 0.003*\"baby\" + 0.003*\"face\" + 0.003*\"chappelle\" + 0.003*\"body\" \n",
      "\n",
      "Topic 2 \n",
      " 0.048*\"audience\" + 0.011*\"fuck\" + 0.004*\"black\" + 0.004*\"police\" + 0.004*\"ngga\" + 0.004*\"everybody\" + 0.004*\"love\" + 0.004*\"na\" + 0.003*\"emotional\" + 0.003*\"twitter\" \n",
      "\n",
      "Topic 3 \n",
      " 0.015*\"fuck\" + 0.010*\"bro\" + 0.010*\"okay\" + 0.008*\"hummus\" + 0.004*\"mom\" + 0.004*\"face\" + 0.004*\"everybody\" + 0.004*\"prayer\" + 0.004*\"ass\" + 0.004*\"baby\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lda_noun_adj = models.LdaModel(corpus=corpus_noun_adj, id2word=id2word_noun_adj, num_topics=4, \n",
    "                               random_state=100, passes=100, alpha='auto', update_every=1, chunksize=100)\n",
    "for topic, topwords in lda_noun_adj.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj_adv(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns, adjectives and adverb.'''\n",
    "    is_noun_adj_adv = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'RB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj_adv(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanaa = pd.DataFrame(data_clean['Transcript'].apply(nouns_adj_adv))\n",
    "datanaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abs</th>\n",
       "      <th>absence</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amer_Mohammed</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dave_Chappelle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drew_Michael_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim_Gaffigan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kathleen</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lousi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4840 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ab  abandoned  ability  able  abolish  abortion  abraham  abs  \\\n",
       "Amer_Mohammed    1          0        0     1        0         0        0    0   \n",
       "Dave_2           0          0        0     0        0         0        0    0   \n",
       "Dave_Chappelle   0          0        0     3        0         0        0    0   \n",
       "Drew_Michael_1   0          0        2     2        0         0        0    3   \n",
       "Drew_Michael_2   0          0        1     5        0         1        0    0   \n",
       "Jim_Gaffigan     0          0        0     3        0         0        0    0   \n",
       "Kathleen         0          0        0     1        0         0        1    0   \n",
       "Lousi            0          0        0     1        1         0        0    0   \n",
       "Phil             0          1        0     0        0         0        0    0   \n",
       "Tom              0          0        0     0        0         0        0    0   \n",
       "\n",
       "                absence  abstract  ...  youngest  youth  youtube  youve  \\\n",
       "Amer_Mohammed         0         0  ...         0      0        0      1   \n",
       "Dave_2                0         0  ...         0      0        1      0   \n",
       "Dave_Chappelle        0         0  ...         0      0        1      0   \n",
       "Drew_Michael_1        0         0  ...         0      0        0      3   \n",
       "Drew_Michael_2        1         0  ...         0      0        0      4   \n",
       "Jim_Gaffigan          0         0  ...         1      1        0      3   \n",
       "Kathleen              0         0  ...         4      0        0      4   \n",
       "Lousi                 0         0  ...         0      0        2      1   \n",
       "Phil                  0         6  ...         0      1        0      5   \n",
       "Tom                   0         1  ...         0      0        1      2   \n",
       "\n",
       "                zimbabwe  zimmerman  zip  ziplining  zone  zoo  \n",
       "Amer_Mohammed          0          0    0          1     0    0  \n",
       "Dave_2                 0          6    0          0     0    0  \n",
       "Dave_Chappelle         0          0    0          0     0    0  \n",
       "Drew_Michael_1         0          0    0          0     0    0  \n",
       "Drew_Michael_2         0          0    0          0     0    0  \n",
       "Jim_Gaffigan           0          0    0          0     0    0  \n",
       "Kathleen               0          0    1          0     0    0  \n",
       "Lousi                  1          0    0          0     0    9  \n",
       "Phil                   0          0    0          0     0    0  \n",
       "Tom                    0          0    0          0     1    0  \n",
       "\n",
       "[10 rows x 4840 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnaa = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvnaa= cvnaa.fit_transform(datanaa['Transcript'])\n",
    "data_dtmnaa = pd.DataFrame(data_cvnaa.toarray(), columns=cvnaa.get_feature_names_out())\n",
    "data_dtmnaa.index = datanaa.index\n",
    "data_dtmnaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusnaa = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmnaa.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordnaa = dict((v, k) for k, v in cvnaa.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.009*\"fuck\" + 0.006*\"trans\" + 0.006*\"black\" + 0.005*\"okay\" + 0.004*\"bitch\" + 0.004*\"gay\" + 0.004*\"everybody\" + 0.004*\"chappelle\" + 0.004*\"bro\" + 0.004*\"baby\" \n",
      "\n",
      "Topic 1 \n",
      " 0.016*\"audience\" + 0.004*\"fuck\" + 0.003*\"chinese\" + 0.003*\"mom\" + 0.003*\"eyes\" + 0.003*\"na\" + 0.003*\"body\" + 0.003*\"house\" + 0.003*\"family\" + 0.003*\"car\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldanaa = models.LdaModel(corpus=corpusnaa, num_topics=2, id2word=id2wordnaa, passes=10)\n",
    "for topic, topwords in ldanaa.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.024*\"audience\" + 0.008*\"trans\" + 0.006*\"fuck\" + 0.005*\"black\" + 0.004*\"chappelle\" + 0.004*\"community\" + 0.004*\"everybody\" + 0.003*\"gay\" + 0.003*\"joke\" + 0.003*\"bitch\" \n",
      "\n",
      "Topic 1 \n",
      " 0.006*\"wife\" + 0.005*\"children\" + 0.004*\"family\" + 0.004*\"house\" + 0.004*\"black\" + 0.004*\"night\" + 0.004*\"dad\" + 0.003*\"pandemic\" + 0.003*\"ass\" + 0.003*\"morning\" \n",
      "\n",
      "Topic 2 \n",
      " 0.011*\"fuck\" + 0.007*\"okay\" + 0.006*\"bro\" + 0.005*\"baby\" + 0.004*\"eyes\" + 0.004*\"chinese\" + 0.004*\"hummus\" + 0.004*\"fucking\" + 0.004*\"body\" + 0.004*\"everybody\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldanaa = models.LdaModel(corpus=corpusnaa, num_topics=3, id2word=id2wordnaa, passes=10)\n",
    "for topic, topwords in ldanaa.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.016*\"fuck\" + 0.011*\"bro\" + 0.009*\"okay\" + 0.009*\"hummus\" + 0.005*\"bitch\" + 0.005*\"ass\" + 0.004*\"everybody\" + 0.004*\"black\" + 0.004*\"baby\" + 0.004*\"police\" \n",
      "\n",
      "Topic 1 \n",
      " 0.008*\"trans\" + 0.006*\"fuck\" + 0.005*\"black\" + 0.005*\"gay\" + 0.004*\"mom\" + 0.004*\"chappelle\" + 0.004*\"everybody\" + 0.004*\"face\" + 0.004*\"community\" + 0.004*\"okay\" \n",
      "\n",
      "Topic 2 \n",
      " 0.059*\"audience\" + 0.010*\"fuck\" + 0.005*\"love\" + 0.004*\"na\" + 0.004*\"everybody\" + 0.004*\"twitter\" + 0.004*\"emotional\" + 0.003*\"thoughts\" + 0.003*\"yelp\" + 0.003*\"chuckles\" \n",
      "\n",
      "Topic 3 \n",
      " 0.005*\"wife\" + 0.005*\"children\" + 0.004*\"chinese\" + 0.004*\"eyes\" + 0.004*\"body\" + 0.004*\"asian\" + 0.003*\"pandemic\" + 0.003*\"cork\" + 0.003*\"dad\" + 0.003*\"family\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldanaa = models.LdaModel(corpus=corpusnaa, num_topics=4, id2word=id2wordnaa, passes=10)\n",
    "for topic, topwords in ldanaa.show_topics():\n",
    "    print(\"Topic\", topic, \"\\n\", topwords, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
